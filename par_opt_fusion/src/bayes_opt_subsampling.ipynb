{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing bayesian optimization explorations at different sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import Colours\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors, cm \n",
    "\n",
    "from math import log, floor, sqrt\n",
    "\n",
    "copper = mpl.cm.copper \n",
    "cNorm  = colors.Normalize(vmin=-0.9, vmax=-0.25)\n",
    "scalarMap = cm.ScalarMappable(norm=cNorm, cmap=copper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities to compute time allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_per_model(pct, algo='svm'): \n",
    "    x = [i for i in range(1,101, 1)] \n",
    " \n",
    "    if algo == 'rf': \n",
    "        nlogn = [i*log(i) for i in x]\n",
    "        return nlogn[99]/nlogn[int(pct*100) - 1]\n",
    "    if algo == 'svm':\n",
    "        n_n = [i*i for i in x]\n",
    "        return n_n[99]/n_n[int(pct*100) - 1] \n",
    "    \n",
    "\n",
    "\n",
    "def budget_division(budget, how='equal', steps=3, lower=0.4):\n",
    "    \n",
    "    def normalizing_factor(lst, budget):\n",
    "        '''sum(lst).X = budget'''\n",
    "        return budget / sum(lst)\n",
    "    \n",
    "    if how == 'equal':\n",
    "        return [int(budget/steps) for _ in range(steps)]\n",
    "    else:\n",
    "        slices = [budget/(1+s) for s in range(steps)]\n",
    "        norm_factor = normalizing_factor(slices, budget)\n",
    "        normalized_slices = [norm_factor*s for s in slices]\n",
    "        \n",
    "        if how == 'linear_asc':\n",
    "            return normalized_slices\n",
    "        if how == 'linear_desc':\n",
    "            return normalized_slices[::-1]\n",
    "        \n",
    "\n",
    "def models_at_sample_size(budget, sample_size, algo):\n",
    "    return int(budget*cost_per_model(sample_size, algo))\n",
    "\n",
    "\n",
    "def size(i, lower=0.4, steps=3):\n",
    "    #i += 1\n",
    "    return lower + i * (1 - lower)/(steps - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Synthetic binary classification dataset.\"\"\"\n",
    "    data, targets = make_classification(\n",
    "        n_samples=5_000,\n",
    "        n_features=22,\n",
    "        n_informative=12,\n",
    "        n_redundant=4, \n",
    "        random_state=0,\n",
    "    )\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to optimize: random forest classifier being score with negative log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_cv(n_estimators, min_samples_split, max_features, data, targets): \n",
    "    estimator = RFC(\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_features=max_features,\n",
    "        random_state=2\n",
    "    ) \n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets,\n",
    "                           scoring='neg_log_loss', cv=3)\n",
    "    return cval.mean()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points to probe in next level. \n",
    "#Something dynamic like sqrt(observations) could do the job but needs additional control mechanisms.\n",
    "n_points = 5\n",
    "\n",
    "from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,\n",
    "                                              ExpSineSquared, DotProduct,\n",
    "                                              ConstantKernel)\n",
    "def points_to_probe(optimizer):\n",
    "    x0_obs = np.array([[res[\"params\"][\"max_features\"]] for res in optimizer.res]) \n",
    "    x1_obs = np.array([[res[\"params\"][\"min_samples_split\"]] for res in optimizer.res])\n",
    "    x2_obs = np.array([[res[\"params\"][\"n_estimators\"]] for res in optimizer.res]) \n",
    "    y_obs = np.array([res[\"target\"] for res in optimizer.res]) \n",
    "     \n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for m, zlow, zhigh in [('o', -50, -25), ('^', -30, -5)]: \n",
    "        ax.scatter(x0_obs, x1_obs, x2_obs, c=scalarMap.to_rgba(y_obs), alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('max_features')\n",
    "    ax.set_ylabel('min_samples_split')\n",
    "    ax.set_zlabel('n_estimators')\n",
    "    plt.colorbar(scalarMap)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "    #plt.matshow(optimizer._gp.L_)\n",
    "    #plt.title('Lower-triangular Cholesky decomposition of cov')\n",
    "    #plt.show() \n",
    "    \n",
    "    idx = y_obs.argsort()[-n_points:][::-1]\n",
    "    probe = [[x0_obs[i], x1_obs[i], x2_obs[i]] for i in idx]\n",
    "    \n",
    "    return probe\n",
    "\n",
    "\n",
    "\n",
    "def optimize_rfc(data, targets, level, cov_function_prior, n_iter=0, bounds=None, to_probe=None):\n",
    "    \"\"\"\n",
    "    level: index + 1 of sample size in [pct0, pct1, .. pctN].\n",
    "    cov_function_prior: definition of cov. function by the gaussian process regression. It's going to be updated every step.\n",
    "    n_iter: number of models to be computed at each sample size. Is constrained by the total budget.\n",
    "    bounds: updated boundaries for hyper param. space.\n",
    "    to_probe: promissing points found in smaller sample sizes.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def rfc_crossval(n_estimators, min_samples_split, max_features): \n",
    "        return rfc_cv(\n",
    "            n_estimators=int(n_estimators),\n",
    "            min_samples_split=float(min_samples_split),\n",
    "            max_features=max(min(max_features, 0.999), 1e-3), \n",
    "            data=data,\n",
    "            targets=targets,\n",
    "        )\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=rfc_crossval,\n",
    "        pbounds={\n",
    "            \"n_estimators\": (10, 250),\n",
    "            \"min_samples_split\": (0.01, 0.999),\n",
    "            \"max_features\": (0.1, 0.999), \n",
    "        },\n",
    "        random_state=1234,\n",
    "        verbose=1\n",
    "    ) \n",
    "    \n",
    "    # model noise in each sample size (level)\n",
    "    optimizer._gp.kernel = cov_function_prior + WhiteKernel(noise_level=0.01/(level + 1))\n",
    "     \n",
    "    if len(to_probe) > 0:\n",
    "        for point in to_probe: \n",
    "            optimizer.probe(\n",
    "                params=point,\n",
    "                lazy=True,\n",
    "                )\n",
    "    \n",
    "    # control structure to constrain compute budget\n",
    "    if level == 1:\n",
    "        init_points = 2 # minimum amount of points to start inference -> randomly generated.\n",
    "    else:\n",
    "        init_points = 0\n",
    "    n_iter -= n_points \n",
    "        \n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter, acq=\"ucb\", kappa=20/level) \n",
    "    \n",
    "    #print(Colours.yellow(f'Prior kernel: {optimizer._gp.kernel}'))\n",
    "    #print(Colours.purple(f'Posterior kernel: {optimizer._gp.kernel_}'))\n",
    "    \n",
    "    cov_function_posterior = optimizer._gp.kernel_\n",
    "\n",
    "    return points_to_probe(optimizer), cov_function_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m--- Optimizing Random Forest: 100 models; budget: 100 --- \u001b[0m\n",
      "|   iter    |  target   | max_fe... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-0.3418  \u001b[0m | \u001b[95m 0.4902  \u001b[0m | \u001b[95m 0.02077 \u001b[0m | \u001b[95m 10.02   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.3349  \u001b[0m | \u001b[95m 0.6166  \u001b[0m | \u001b[95m 0.01651 \u001b[0m | \u001b[95m 10.02   \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m-0.2919  \u001b[0m | \u001b[95m 0.999   \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 250.0   \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_data()\n",
    "\n",
    "lower = 1\n",
    "steps = 2\n",
    "budget = 200\n",
    "\n",
    "bounds = None\n",
    "to_probe = []\n",
    "cov_function_prior = Matern(nu=2.5) + WhiteKernel(noise_level=0.01)\n",
    "\n",
    "plt.figure()\n",
    "for level, b in enumerate(budget_division(budget, how='equal', steps=steps, lower=lower)):\n",
    "    sample_size = size(level, lower, steps) \n",
    "    n_iter = models_at_sample_size(b, sample_size, 'rf')\n",
    "    \n",
    "    rows = int(len(data) * sample_size)\n",
    "    idx = np.random.choice(len(data), rows, replace=False)\n",
    "    sampled_X = data[idx,:]\n",
    "    sampled_y = targets[idx]\n",
    "\n",
    "    print(Colours.green(f\"--- Optimizing Random Forest: {n_iter} models; budget: {b} --- \"))\n",
    "    to_probe, cov_function_posterior = optimize_rfc(sampled_X, sampled_y, level + 1, cov_function_prior, n_iter, bounds, to_probe)\n",
    "    \n",
    "    cov_function_prior = cov_function_posterior \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
