{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing bayesian optimization explorations at different sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import Colours\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors, cm \n",
    "\n",
    "from math import log, floor, sqrt\n",
    "\n",
    "copper = mpl.cm.copper \n",
    "cNorm  = colors.Normalize(vmin=-1, vmax=0)\n",
    "scalarMap = cm.ScalarMappable(norm=cNorm, cmap=copper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities for compute time allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_per_model(pct, algo='svm'): \n",
    "    x = [i for i in range(1,101, 1)] \n",
    " \n",
    "    if algo == 'rf': \n",
    "        nlogn = [i*log(i) for i in x]\n",
    "        return nlogn[99]/nlogn[int(pct*100) - 1]\n",
    "    if algo == 'svm':\n",
    "        n_n = [i*i for i in x]\n",
    "        return n_n[99]/n_n[int(pct*100) - 1] \n",
    "    \n",
    "\n",
    "\n",
    "def budget_division(budget, how='equal', steps=3, lower=0.4):\n",
    "    \n",
    "    def normalizing_factor(lst, budget):\n",
    "        '''sum(lst).X = budget'''\n",
    "        return budget / sum(lst)\n",
    "    \n",
    "    if how == 'equal':\n",
    "        return [int(budget/steps) for _ in range(steps)]\n",
    "    else:\n",
    "        slices = [budget/(1+s) for s in range(steps)]\n",
    "        norm_factor = normalizing_factor(slices, budget)\n",
    "        normalized_slices = [norm_factor*s for s in slices]\n",
    "        \n",
    "        if how == 'linear_asc':\n",
    "            return normalized_slices\n",
    "        if how == 'linear_desc':\n",
    "            return normalized_slices[::-1]\n",
    "        \n",
    "\n",
    "def models_at_sample_size(budget, sample_size, algo):\n",
    "    return int(budget*cost_per_model(sample_size, algo))\n",
    "\n",
    "\n",
    "def size(i, lower=0.4, steps=3):\n",
    "    #i += 1\n",
    "    return lower + i * (1 - lower)/(steps - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Synthetic binary classification dataset.\"\"\"\n",
    "    data, targets = make_classification(\n",
    "        n_samples=5_000,\n",
    "        n_features=22,\n",
    "        n_informative=12,\n",
    "        n_redundant=4, \n",
    "        random_state=0,\n",
    "    )\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to optimize: random forest classifier being score with negative log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_cv(n_estimators, min_samples_split, max_features, data, targets): \n",
    "    estimator = RFC(\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_features=max_features,\n",
    "        random_state=2\n",
    "    ) \n",
    "    \n",
    "    cval = cross_val_score(estimator, data, targets,\n",
    "                           scoring='neg_log_loss', cv=3)\n",
    "    return cval.mean()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 5 # points to probe in next level\n",
    "\n",
    "def points_to_explore(optimizer):\n",
    "    x0_obs = np.array([[res[\"params\"][\"max_features\"]] for res in optimizer.res]) \n",
    "    x1_obs = np.array([[res[\"params\"][\"min_samples_split\"]] for res in optimizer.res])\n",
    "    x2_obs = np.array([[res[\"params\"][\"n_estimators\"]] for res in optimizer.res]) \n",
    "    y_obs = np.array([res[\"target\"] for res in optimizer.res]) \n",
    "     \n",
    " \n",
    "    fig, ax = plt.subplots(3, 1,figsize=(3,9))\n",
    "    ax[0].scatter(x0_obs, x1_obs, c=scalarMap.to_rgba(y_obs))   \n",
    "    ax[0].set_xlabel(\"max_features\")\n",
    "    ax[0].set_ylabel(\"min_samples_split\")\n",
    "    \n",
    "    ax[1].scatter(x0_obs, x2_obs, c=scalarMap.to_rgba(y_obs))\n",
    "    ax[1].set_xlabel(\"max_features\")\n",
    "    ax[1].set_ylabel(\"n_estimators\")\n",
    "    \n",
    "    ax[2].scatter(x1_obs, x2_obs, c=scalarMap.to_rgba(y_obs))\n",
    "    ax[2].set_xlabel(\"min_samples_split\")\n",
    "    ax[2].set_ylabel(\"n_estimators\")\n",
    "    fig.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "    plt.matshow(optimizer._gp.L_)\n",
    "    plt.show()\n",
    "    \n",
    "    #n_points = floor(sqrt(n_iter))\n",
    "    \n",
    "    idx = y_obs.argsort()[-n_points:][::-1]\n",
    "    to_explore = [[x0_obs[i], x1_obs[i], x2_obs[i]] for i in idx]\n",
    "    \n",
    "    return to_explore\n",
    "\n",
    "\n",
    "\n",
    "def optimize_rfc(data, targets, level, n_iter=0, bounds=None, probe=None):\n",
    "    \"\"\"Apply Bayesian Optimization to Random Forest parameters.\"\"\"\n",
    "    def rfc_crossval(n_estimators, min_samples_split, max_features): \n",
    "        return rfc_cv(\n",
    "            n_estimators=int(n_estimators),\n",
    "            min_samples_split=float(min_samples_split),\n",
    "            max_features=max(min(max_features, 0.999), 1e-3), \n",
    "            data=data,\n",
    "            targets=targets,\n",
    "        )\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=rfc_crossval,\n",
    "        pbounds={\n",
    "            \"n_estimators\": (10, 250),\n",
    "            \"min_samples_split\": (0.01, 0.999),\n",
    "            \"max_features\": (0.1, 0.999), \n",
    "        },\n",
    "        random_state=1234,\n",
    "        verbose=2\n",
    "    ) \n",
    "    optimizer._gp.kernel = Matern(nu=2.5) + WhiteKernel(noise_level=0.1/level)\n",
    "    \n",
    "    if len(params) > 0:\n",
    "        for p in params: \n",
    "            optimizer.probe(\n",
    "            params=p,\n",
    "            lazy=True,\n",
    "            )\n",
    "    \n",
    "    if level == 1:\n",
    "        init_points = 2\n",
    "        n_iter -= init_points\n",
    "    else:\n",
    "        init_points = 0\n",
    "        n_iter -= n_points \n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter, acq=\"ucb\", kappa=20/level) \n",
    "    \n",
    "    return points_to_explore(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m--- Optimizing Random Forest: 72 models; budget:16 --- \u001b[0m\n",
      "|   iter    |  target   | max_fe... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.649   \u001b[0m | \u001b[0m 0.2722  \u001b[0m | \u001b[0m 0.6253  \u001b[0m | \u001b[0m 115.1   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.6927  \u001b[0m | \u001b[0m 0.806   \u001b[0m | \u001b[0m 0.7814  \u001b[0m | \u001b[0m 75.42   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.3945  \u001b[0m | \u001b[95m 0.2883  \u001b[0m | \u001b[95m 0.03816 \u001b[0m | \u001b[95m 115.3   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.6131  \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 0.4403  \u001b[0m | \u001b[0m 153.6   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.6928  \u001b[0m | \u001b[0m 0.7878  \u001b[0m | \u001b[0m 0.904   \u001b[0m | \u001b[0m 19.49   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.6927  \u001b[0m | \u001b[0m 0.85    \u001b[0m | \u001b[0m 0.9176  \u001b[0m | \u001b[0m 119.1   \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_data() \n",
    "\n",
    "level = 1\n",
    "lower = 0.3\n",
    "steps = 3\n",
    "budget = 50\n",
    "\n",
    "bounds = None\n",
    "params = []\n",
    "\n",
    "plt.figure()\n",
    "for i, b in enumerate(budget_division(budget, how='equal', steps=steps, lower=lower)):\n",
    "    sample_size = size(i, lower, steps) \n",
    "    n_iter = models_at_sample_size(b, sample_size, 'rf')\n",
    "    \n",
    "    rows = int(len(data) * sample_size)\n",
    "    idx = np.random.choice(len(data), rows, replace=False) \n",
    "    sampled_X = data[idx,:]\n",
    "    sampled_y = targets[idx]\n",
    "    data, targets = get_data() \n",
    "\n",
    "    print(Colours.green(f\"--- Optimizing Random Forest: {n_iter} models; budget:{b} --- \"))\n",
    "    params = optimize_rfc(sampled_X, sampled_y, level, n_iter, bounds, params)   \n",
    "    level += 1\n",
    "#plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
