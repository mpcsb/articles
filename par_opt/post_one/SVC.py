from sklearn.svm import SVR
from collections import defaultdict

import numpy as np
from sklearn.model_selection import cross_val_score 
from sklearn.ensemble import RandomForestRegressor as RFR 
from sklearn.datasets import load_boston
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  

X, y = load_boston(return_X_y=True) 

def svccv(target, train_data, C, gamma):
    val = cross_val_score(
        SVR(kernel='rbf', cache_size=4000, C=C, gamma=gamma),
        train_data, target,  
        cv=6, #njobs=7
    ).mean()
    return val

# def rfrcv(target, train_data, param1, param2):
#     val = cross_val_score(
#         RFR(min_samples_split=param1, 
#             min_samples_leaf=param2,
#             criterion='mse',
#             random_state=2,
#         ),
#         train_data, target, 
#         cv=6, n_jobs=7
#     ).mean()
#     return val
 
def lerp(a, b, f):
    return a + f * (b - a)

params = {'c': (0.001, 50),
         'gamma': (0.001, 0.3), 
        }

#%% exploring variation in hyperparameter over sample size
size_grid = list(range(100, len(X), 100)) 
# size_grid = [100, 200, 300, 400, 500]
steps = 20

grid_search = dict()
for size in size_grid:
    print(size, len(X))
    grid_search[size] = list() 
    
    idx = np.random.randint(len(X), size=size)
    train_data = X[idx,:]
    target = y[idx] 
     
    for n1 in range(steps): 
        print(n1)
        param1 = round(lerp(params['c'][0], params['c'][1], n1/(steps-1)), 3) 
        for n2 in range(steps): 
            param2 = round(lerp(params['gamma'][0], params['gamma'][1], n2/(steps-1)), 3)         
            val = svccv(target=target, train_data=train_data, C=param1, gamma=param2)   
            grid_search[size].append([val, param1, param2]) 
 
#%% 3d view is more appealing for the parameter space dimension
import matplotlib as mpl

colormap = mpl.cm.autumn 
colorst = [colormap(50*i) for i in range(len(size_grid))]
title_font = {'fontname':'Arial', 'size':'26', 'color':'black', 'weight':'normal',
              'verticalalignment':'bottom'}

for i, size in enumerate(size_grid):
    d = grid_search[size]  
    x, y, z = list(map(list, zip(*d))) 

    x = np.array(x) 
    y = np.array(y) 
    z = np.array(z) 
    
    a = 45      
    for degree in range(a, a + 360, 360):
        print(i) 
        ax = Axes3D(plt.figure(figsize=(15, 15)))
        ax.plot_trisurf(y, z, x, alpha=0.6, color=colorst[i])
        # ax.scatter(y, z, x, s=10, marker='o', c='red')
        ax.view_init(25, degree)   
        # ax._axis3don = False
        # ax.w_zaxis.line.set_lw(0.)
        # ax.set_zticks([])
        ax.set_xlabel('C')
        ax.set_ylabel('gamma')
        # ax.text2D(0.05, 0.45, "2D Text", transform=ax.transAxes)

        plt.title('Sample size: ' + str(round(size/5))+ '%', **title_font) 
        plt.show()
        
'''
As in the one parameter case, the surface generated by the performance of the 
model by the parameter tuple is pretty similar as we vary the sample size.
'''

#%%

200**2 / 500**2
 